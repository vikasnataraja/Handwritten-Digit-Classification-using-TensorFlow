{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Example\n",
    "\n",
    "Build a convolutional neural network with TensorFlow.\n",
    "\n",
    "This example shows how to build a convolutional neural network classifier to classify digits 0-9 in the MNIST dataset.\n",
    "\n",
    "- Author: Aymeric Damien\n",
    "- Modified by: Vikas Nataraja\n",
    "- Project: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Overview\n",
    "\n",
    "![CNN](http://personal.ie.cuhk.edu.hk/~ccloy/project_target_code/images/fig3.png)\n",
    "\n",
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. \n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# future is imported to allow the use of different versions of Python\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "# TensorFlow is the Deep Learning library created by Google that is used to build the neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the dataset \n",
    "\n",
    "* Training images are loaded into `train_images` as numpy arrays, corresponding labels (ground truth) are read into another numpy array called `train_labels`\n",
    "* A similar procedure is followed for test images.\n",
    "* Finally, the arrays are cast to 32 bit float because TensorFlow takes in float images\n",
    "\n",
    "* `train_images` will be a 3D array - (num_of_images, height, width)\n",
    "* `train_labels` will be a 1D array of 0-9 because we are trying to predict digits 0-9. So the labels are solutions/ground truth to the coresponding train_images\n",
    "* `test_images` will be a 3D array - (num_of_images, height, width)\n",
    "* `test_labels` will be a 1D array of 0-9 because we are trying to predict digits 0-9. So the labels are solutions/ground truth to the corresponding test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = np.float32(train_images)\n",
    "test_images = np.float32(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of training images: ',train_images.shape[0])\n",
    "print('Each training image is of size: ',train_images.shape[1:])\n",
    "print('Number of training labels: ',train_labels.shape[0],'\\n')\n",
    "print('Number of test images: ',test_images.shape[0])\n",
    "print('Each test image is of size: ',test_images.shape[1:])\n",
    "print('Number of test labels: ',test_images.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the dataset - what does the MNIST dataset look like?\n",
    "\n",
    "* Here, we visualize the training set. Each time this cell block is run, random images from the training set will be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "fig=plt.figure(figsize=(16, 16))\n",
    "columns = 4\n",
    "rows = 5\n",
    "for i in range(1, columns*rows +1):\n",
    "    # show random images from the dataset\n",
    "    random_range = randint(0, train_images.shape[0]-1)\n",
    "    img = train_images[random_range]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(img,cmap='gray')\n",
    "    plt.title('Ground truth label = {}'.format(train_labels[random_range]))\n",
    "    plt.axis('off')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model directory\n",
    "\n",
    "This is where the entire model will be saved on your computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# CHANGE THIS DIRECTORY TO ANY DIRECTORY ON YOUR SYSTEM!!!\n",
    "# This is where the model will be saved after it is run\n",
    "#########################################################\n",
    "model_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "These are the parameters that can be tuned depending on how training goes. For example, \n",
    "\n",
    "    * If the model is too slow to train, decrease the batch size.\n",
    "    * If the loss is still high after training, increase the number of steps\n",
    "    * If the model is learning too slowly or is not converging faster, increase the learning rate.\n",
    "\n",
    "        \n",
    "**Batch size** \n",
    "- The number of images to be taken and trained before updating the weights.\n",
    "- This is done to reduce the memory consumption. Instead of training all images at the same time, we do it batch-by-batch.\n",
    "- Batch sizes are usually in powers of 2 e.g 16, 32, 64, 128 ...\n",
    "- For example if your total number of training images = 2000 and batch size is 128 then we get 15 full batches and the final batch will have the remaining images.\n",
    "- Higher batch size almost always gives better accuracies but will be computationally slow\n",
    "\n",
    "**Learning rate**\n",
    "- It is a number between 0 and 1\n",
    "- Dictates how fast your optimization moves. A popular optimization algorithm is gradient descent.\n",
    "- Learning rates are usually set in tenths like 0.1, 0.01 etc,. It is a good idea to start with a very low value and gradually increase\n",
    "\n",
    "**Number of steps**\n",
    "- Indicates how many epochs to train for.\n",
    "- Generally speaking, it is common to train for 100,000 or even 500,000 epochs. For this example we choose a small number - 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "dropout = 0.25 # Dropout, probability to drop a unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-tunable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the layers in the network\n",
    "\n",
    "* The convolutional layer, max pooling layer, fully connected layer are all defined here.\n",
    "* This is essentially the bulk of the work in creating your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        \n",
    "        ################################################################\n",
    "        # INPUT LAYER\n",
    "        ################################################################\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input becomes 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "        \n",
    "        #################################################################\n",
    "        # CONVOLUTION LAYER - 1\n",
    "        #################################################################\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        #################################################################\n",
    "        # CONVOLUTION LAYER - 2\n",
    "        #################################################################\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "        \n",
    "        #################################################################\n",
    "        # FULLY-CONNECTED LAYER - 1\n",
    "        #################################################################\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model function\n",
    "\n",
    "* The model function rounds out the network\n",
    "* It combines your layers, defines the loss functions, defines the optimizer, accuracy and error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the estimator\n",
    "\n",
    "* Estimator is optional. It boxes your entire model so everything can be accessed through it\n",
    "* It also adds the train and test functions\n",
    "* The other way to do it would be to define and then call the training and testing functions individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn,model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "* This takes time. Depending on the dataset, the network intricacies and the epochs, training time could take anywhere between minutes to days or even weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': train_images}, y=train_labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model on training set (Optional)\n",
    "\n",
    "* The model has been trained now. Test the model's performance on training set itself.\n",
    "* This is done to evaluate how well the model will do on images it has definitely already seen.\n",
    "* Ideally, this should be 100% accurate but >90% is usually sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': train_images}, y=train_labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model on test set\n",
    "\n",
    "* Now that the model has been trained, you can test it on new images i.e your test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': test_images}, y=test_labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict single images\n",
    "n_images = 21\n",
    "\n",
    "# Prepare the input data\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': test_images[:n_images]},shuffle=False)\n",
    "\n",
    "# Use the model to predict the images class\n",
    "predictions = list(model.predict(input_fn))\n",
    "\n",
    "# Display\n",
    "fig=plt.figure(figsize=(16, 16))\n",
    "columns = 4\n",
    "rows = 5\n",
    "for i in range(1, columns*rows +1):\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(test_images[i],cmap='gray')\n",
    "    plt.title('Predicted label = {}'.format(predictions[i]))\n",
    "    plt.axis('off')   \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
